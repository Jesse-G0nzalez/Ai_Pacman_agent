<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Portfolio for 41118 AI in robotics class at UTS.">
  <meta name="keywords" content="41118">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AI-Powered Pac-Man Agent: Reinforcement Learning in Action</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AI-Powered Pac-Man Agent: Reinforcement Learning in Action</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Jesse Gonzalez,</span>
            <span class="author-block">Arhma Baig,</span>
            <span class="author-block">Megha Martin,</span>
            <span class="author-block">Sophia Kuhnert</span>
          </div>

          <div class="is university">
            <span class="author-block"><sup>1</sup>University Technology Sydney,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#video"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Jesse-G0nzalez/Ai_Pacman_agent"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/Jesse-G0nzalez/Ai_Pacman_agent/tree/main/src/logs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/placeholder.png"
           class="teaser-image"
           alt="Project Teaser Placeholder"/>
      <h2 class="subtitle has-text-centered">
        This placeholder image will be replaced with a custom project diagram or architecture overview. 
        Use this space to describe the flow of your reinforcement learning framework or system pipeline.
      </h2>
    </div>
  </div>
</section> -->


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Team Members</h2>

    <div id="team-carousel" 
         class="carousel carousel-animated carousel-animate-slide"
         data-size="4"        <!-- show up to 4 at once -->
         data-autoplay="false"
         data-navigation="true"
         data-pagination="false">

      <div class="carousel-container">
        <!-- slide 1 -->
        <div class="carousel-item">
          <figure class="image is-128x128">
            <img src="./static/images/team/jesse.PNG" alt="Jesse Gonzalez">
          </figure>
          <p class="subtitle is-5 has-text-centered mt-2">Jesse Gonzalez</p>
        </div>

        <!-- slide 2 -->
        <div class="carousel-item">
          <figure class="image is-128x128">
            <img src="./static/images/team/arhma.jpg" alt="Arhma Baig">
          </figure>
          <p class="subtitle is-5 has-text-centered mt-2">Arhma Baig</p>
        </div>

        <!-- slide 3 -->
        <div class="carousel-item">
          <figure class="image is-128x128">
            <img src="./static/images/team/megha.jpg" alt="Megha Martin">
          </figure>
          <p class="subtitle is-5 has-text-centered mt-2">Megha Martin</p>
        </div>

        <!-- slide 4 -->
        <div class="carousel-item">
          <figure class="image is-128x128">
            <img src="./static/images/team/sophia.jpg" alt="Sophia Kuhnert">
          </figure>
          <p class="subtitle is-5 has-text-centered mt-2">Sophia Kuhnert</p>
        </div>
      </div>

      <!-- these arrows get auto-injected by Bulma-Carousel when data-navigation="true" -->
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This project showcases the development of an AI agent capable of playing Pac-Man using Deep Q-Networks (DQN), as part of the 41118 AI in Robotics subject at UTS. The agent is trained within a custom Gymnasium environment that replicates classic Pac-Man dynamics, including pellet collection, ghost evasion, and reward-based navigation.
          </p>
          <p>
            We implemented a convolutional neural network to approximate Q-values, leveraging techniques such as epsilon-greedy exploration, experience replay, and target network updates to ensure learning stability and performance improvement over time. Through iterative training and evaluation, the agent progressively learned to make strategic decisions to maximize its score while avoiding game-ending collisions.
          </p>
          <p>
            The project demonstrates the practical application of reinforcement learning algorithms in a controlled game environment, providing insights into state-action value learning, policy improvement, and real-time agent behaviour. A demonstration video and full source code are included below.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


    <!-- Video Section -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 id="video" class="title is-3">Video</h2>

        <p class="mb-4">
          The following video demonstrates the AI agent playing Pac-Man using reinforcement learning. It showcases the agent's learning progress, strategy, and gameplay behaviour.
        </p>

        <div class="content has-text-centered">
          <video id="project-video"
                 controls
                 preload="metadata"
                 playsinline
                 width="100%">
            <source src="./static/videos/double_dqn_crop.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
    <!--/ Video Section -->

<!-- 2Ã—2 Grid of Charts -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">All Training Charts</h2>
    <div class="columns is-multiline">
      
      <!-- 1st Chart -->
      <div class="column is-half">
        <figure class="image is-4by3">
          <img src="./static/images/data/exploration_totalreward.png"
               alt="Reward Convergence Plot">
        </figure>
      </div>
      
      <!-- 2nd Chart -->
      <div class="column is-half">
        <figure class="image is-4by3">
          <img src="./static/images/data/learing_curve.png"
               alt="Loss vs Mean Q-value">
        </figure>
      </div>
      
      <!-- 3rd Chart -->
      <div class="column is-half">
        <figure class="image is-4by3">
          <img src="./static/images/data/length_reward.png"
               alt="Exploration Rate vs Total Reward">
        </figure>
      </div>
      
      <!-- 4th Chart -->
      <div class="column is-half">
        <figure class="image is-4by3">
          <img src="./static/images/data/meanq_loss.png"
               alt="Episode Length vs Total Reward">
        </figure>
      </div>
      
    </div>
  </div>
</section>


   <!-- Results discussion and reflection -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results Discussion and Reflection</h2>

        <div class="content has-text-justified">
          <p>
            The AI agent demonstrated clear learning progress over the course of training. Initially, the agent chose random actions and often collided with ghosts or failed to collect pellets efficiently. As training progressed, the Deep Q-Network (DQN) began to favor safer, more rewarding paths, showing an increased ability to survive longer, avoid threats, and prioritize pellet collection.
          </p>
          <p>
            Evaluation of the trained model on unseen test episodes showed a noticeable improvement in average score and survival time compared to the untrained baseline. The reward convergence plot illustrated steady learning, with the total episode reward stabilizing as the epsilon-greedy exploration policy decayed.
          </p>
          <p>
            However, several limitations were observed. The agent sometimes exhibited overly cautious behavior, avoiding ghosts at the cost of missing nearby rewards. The training process was also sensitive to hyperparameters such as learning rate, replay buffer size, and reward shaping. Additionally, the model was evaluated in a relatively simple and deterministic environment, which may not generalize well to more complex or randomized game scenarios.
          </p>
          <p>
            Future improvements could involve extending the environment to include randomized ghost behavior, implementing prioritized experience replay, or switching to more advanced methods like Double DQN or Dueling DQN. Further, training the agent using a curriculum-based approach might help improve performance in stages of increasing difficulty.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

 <!--/ Results discussion and reflection -->
  
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website was adapted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies project template</a>,
            originally licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
            Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Modifications have been made for educational use in the 41118 AI in Robotics course at UTS.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
